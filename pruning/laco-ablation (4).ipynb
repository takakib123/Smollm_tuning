{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch.nn as nn\nimport numpy as np\nimport os\nimport random\nimport time\nimport datetime\nimport csv\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom copy import deepcopy\nimport gc\nimport pandas as pd\n\n# --- Experiment Configuration ---\n\n# Reproducibility\nRANDOM_SEED = 42\n\n# Device Configuration\n# Use two GPUs if available for base model and compressed model\ndevice_main = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# device_main = torch.device('cpu')\n\ndevice_comp = torch.device(\"cuda:1\" if torch.cuda.device_count() > 1 else device_main)\n# device_comp = torch.device('cpu')\n\n# Dataset Configuration\nSMOKE_TEST = True # Set to True to use a smaller subset of the data for quick tests\nSMOKE_TEST_SAMPLES = 5 # Number of samples for smoke test perplexity evaluation\n\n# Logging and Saving\nRESULTS_FILE = 'results.csv'\nSAVE_MODELS = False # Set to True to save the weights of each trained model\nMODEL_SAVE_DIR = 'saved_models'\n\n# --- Hyperparameter Grid ---\n# Define different configurations to test in the experimentation loop.\n# Add or remove dictionaries to change the experiments.\nhyperparameter_grid = [\n    {\n        'model_path': 'lmsys/vicuna-7b-v1.5',\n        'merge_layers': 4,\n        'interval': 2,\n        'highest_lay': 20, # Should be adapted based on model architecture\n        'lowest_lay': 6,\n        'threshold': 0.65,\n    },\n    {\n        'model_path': 'HuggingFaceTB/SmolLM2-135M-Instruct',\n        'merge_layers': 4,\n        'interval': 2,\n        'highest_lay': 39,\n        'lowest_lay': 0,\n        'threshold': 0.75, # Higher threshold\n    },\n    {\n        'model_path': 'lmsys/vicuna-7b-v1.5',\n        'merge_layers': 3, # Fewer layers merged at a time\n        'interval': 3,     # Wider interval\n        'highest_lay': 39,\n        'lowest_lay': 0,\n        'threshold': 0.65,\n    },\n]\n\n# # Create directory for saving models if it doesn't exist\n\n\n# if SAVE_MODELS and not os.path.exists(MODEL_SAVE_DIR):\n#     os.makedirs(MODEL_SAVE_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T12:39:34.875284Z","iopub.execute_input":"2025-08-22T12:39:34.875675Z","iopub.status.idle":"2025-08-22T12:39:39.137616Z","shell.execute_reply.started":"2025-08-22T12:39:34.875655Z","shell.execute_reply":"2025-08-22T12:39:39.136712Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def cal_last_hidden_sim_from_saved(saved_hidden_states, model, tokenizer, sents):\n    \"\"\"\n    Calculates mean cosine similarity between *saved* last hidden states\n    (from a previous model) and last hidden states from `model` for given sentences.\n\n    Parameters:\n        saved_hidden_states: List[torch.Tensor] of last hidden states, one per sentence\n        model: Hugging Face-style model to compare against\n        tokenizer: Tokenizer for the model\n        sents: List[str] â€” should be same sentences used to generate saved_hidden_states\n\n    Returns:\n        float: mean cosine similarity\n    \"\"\"\n\n    hs2 = extract_last_hidden_states(model, tokenizer, sents)\n\n    sims = []\n    for h1, h2 in zip(saved_hidden_states, hs2):\n        v1 = h1.flatten()\n        v2 = h2.flatten()\n        sim = torch.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0)).item()\n        sims.append(sim)\n\n    return float(np.mean(sims))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T12:39:47.648598Z","iopub.execute_input":"2025-08-22T12:39:47.649418Z","iopub.status.idle":"2025-08-22T12:39:47.654627Z","shell.execute_reply.started":"2025-08-22T12:39:47.649387Z","shell.execute_reply":"2025-08-22T12:39:47.653976Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def set_seed(seed):\n    \"\"\"Sets the random seed for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef create_model(model_path, device):\n    \"\"\"Creates and loads a pretrained model and tokenizer.\"\"\"\n    print(f\"Loading model: {model_path}\")\n    model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16).to(device)\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    return model, tokenizer\n\ndef get_model_size(model: nn.Module):\n    \"\"\"Calculates the number of parameters in a model.\"\"\"\n    num_elements = 0\n    for param in model.parameters():\n        num_elements += param.numel()\n    return num_elements\n\ndef evaluate_perplexity(model, tokenizer, smoke_test=False, smoke_test_samples=5):\n    \"\"\"Computes perplexity on the WikiText-2 test split.\"\"\"\n    model.eval()\n    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n    text = \"\\n\\n\".join(ds[\"text\"])\n    tokens = tokenizer(text, return_tensors=\"pt\")\n    input_ids = tokens.input_ids.to(model.device)\n\n    nsamples = smoke_test_samples if smoke_test else 40\n    seq_len = 2048\n    \n    # Check if the dataset is long enough\n    if input_ids.size(1) < nsamples * seq_len:\n        print(f\"Warning: Dataset too small for nsamples={nsamples} and seq_len={seq_len}. Reducing nsamples.\")\n        nsamples = input_ids.size(1) // seq_len\n        if nsamples == 0:\n            print(\"Error: Not enough data to evaluate perplexity.\")\n            return float('nan')\n\n\n    loss_fct = nn.CrossEntropyLoss()\n    nlls = []\n    \n    for i in tqdm(range(nsamples), desc=\"Perplexity Evaluation\"):\n        start, end = i * seq_len, (i + 1) * seq_len\n        batch = input_ids[:, start:end]\n        with torch.no_grad():\n            logits = model(batch).logits\n\n        shift_logits = logits[:, :-1, :].contiguous()\n        shift_labels = batch[:, 1:]\n        loss = loss_fct(\n            shift_logits.view(-1, shift_logits.size(-1)),\n            shift_labels.reshape(-1)\n        )\n        nlls.append(loss * seq_len)\n\n    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * seq_len))\n    return ppl.item()\n\n\ndef extract_last_hidden_states(model, tokenizer, sents):\n    \"\"\"\n    Runs the given model on a list of sentences and returns the\n    final hidden states (detached to CPU) for each sentence.\n\n    Parameters:\n        model: Hugging Face-style model with `model.layers` attribute\n        tokenizer: Corresponding tokenizer\n        sents: List of strings\n\n    Returns:\n        List[torch.Tensor]: one hidden state tensor per sentence\n    \"\"\"\n    hidden_states = []\n\n    # Temporary container for hook capture\n    hidden_container = {}\n\n    def hook_fn(module, input, output):\n        # Assumes output is a tuple (hidden_states, ...)\n        hidden_container[\"h\"] = output[0].detach().cpu()\n\n    # Register on last transformer block (LLaMA/Vicuna style)\n    hook = model.model.layers[-1].register_forward_hook(hook_fn)\n\n    try:\n        for s in sents:\n            enc = tokenizer(s, return_tensors=\"pt\")\n            inputs = enc.input_ids.to(model.device)\n            with torch.no_grad():\n                _ = model(inputs)\n            hidden_states.append(hidden_container[\"h\"])\n    finally:\n        hook.remove()\n\n    return hidden_states\n\n\ndef cal_last_hidden_sim(model1, model2, tokenizer, sents):\n    \"\"\"\n    Calculates mean cosine similarity between final hidden layers\n    of two models for given sentences.\n\n    Uses extract_last_hidden_states() to get embeddings in a memory-safe way.\n    \"\"\"\n    hs1 = extract_last_hidden_states(model1, tokenizer, sents)\n    hs2 = extract_last_hidden_states(model2, tokenizer, sents)\n\n    sims = []\n    for h1, h2 in zip(hs1, hs2):\n        # Flatten to 1D\n        v1 = h1.flatten()\n        v2 = h2.flatten()\n        sim = torch.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0)).item()\n        sims.append(sim)\n\n    return float(np.mean(sims))\n\n\n\ndef compress_model(base_model, tokenizer, config):\n    \"\"\"Applies the layer-merging compression algorithm to a model.\"\"\"\n    \n    sents = ['Mouron () is a commune in the Arde',\n             'The 81st Mechanised Brigade () is a mechanised brigade of the Romanian Land Force',\n             'There are 18 National Natural Landmarks in the U.S. state of Washington, out of nearly',\n             'Torreorgaz is a municipality in the',\n             'Copa Libertadores 1973 was won by defending champions Independiente of A']\n\n    last_hidden_states = extract_last_hidden_states(base_model, tokenizer, sents)\n\n    # Save to disk for future use\n    torch.save(last_hidden_states, \"last_hidden_states.pt\")\n\n    # model_to_compress = deepcopy(base_model).to(device_comp)\n    model_to_compress = base_model\n    del base_model\n    gc.collect()\n    torch.cuda.empty_cache()\n    print('Compressed model loaded')\n    \n    lay = config['highest_lay'] - config['merge_layers']\n    \n    while lay >= config['lowest_lay']:\n        print(f\"Attempting to merge at layer: {lay}\")\n        \n        # Ensure we don't go out of bounds\n        if lay >= len(model_to_compress.model.layers):\n            lay = len(model_to_compress.model.layers) - 1 - config['merge_layers']\n            if lay < config['lowest_lay']: break\n            continue\n\n        tmp_merged_model = merge_layers_return_model(\n            model_to_compress, lay, config['merge_layers'] - 1\n        ).to(device_main)\n        \n        torch.cuda.empty_cache()\n        sim_value = cal_last_hidden_sim_from_saved(\n            last_hidden_states,\n            tmp_merged_model,\n            tokenizer,\n            sents\n        )\n        print(f\"Similarity after potential merge at layer {lay}: {sim_value:.4f}\")\n\n        if sim_value > config['threshold']:\n            print(f\"Merge accepted. New layer count: {len(tmp_merged_model.model.layers)}\")\n            model_to_compress = tmp_merged_model\n            lay -= config['interval']\n        else:\n            print(\"Merge rejected.\")\n            lay -= 1\n            \n        del tmp_merged_model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    model_to_compress.to(0)       \n    # Update model config with the new number of layers\n    model_to_compress.config.num_hidden_layers = len(model_to_compress.model.layers)\n    return model_to_compress\n\n\n# def merge_layers_return_model(model, merge_base_lay, merge_layer_num):\n#     \"\"\"Helper function to perform the actual layer weight merging.\"\"\"\n#     # model.to('cpu')\n#     # torch.cuda.empty_cache()\n#     # model_copy = deepcopy(model).to(0)\n#     model.to('cpu')\n    \n#     model_copy = deepcopy(model).to(1)\n#     # Ensure merge_layer_num doesn't exceed available layers\n#     merge_layer_num = min(merge_layer_num, len(model.model.layers) - merge_base_lay - 1)\n#     if merge_layer_num < 0: return model_copy # Nothing to merge\n\n#     for diff_lay in range(merge_base_lay + 1, merge_base_lay + 1 + merge_layer_num):\n#         # MLP layers\n#         model_copy.model.layers[merge_base_lay].mlp.gate_proj.weight.data.add_(model.model.layers[diff_lay].mlp.gate_proj.weight.data.clone().to(1) - model_copy.model.layers[merge_base_lay].mlp.gate_proj.weight.data)\n#         model_copy.model.layers[merge_base_lay].mlp.down_proj.weight.data.add_(model.model.layers[diff_lay].mlp.down_proj.weight.data.clone().to(1) - model_copy.model.layers[merge_base_lay].mlp.down_proj.weight.data)\n#         model_copy.model.layers[merge_base_lay].mlp.up_proj.weight.data.add_(model.model.layers[diff_lay].mlp.up_proj.weight.data.clone().to(1) - model_copy.model.layers[merge_base_lay].mlp.up_proj.weight.data)\n\n\n#         print('can copy')\n#         # Attention layers\n#         model_copy.model.layers[merge_base_lay].self_attn.q_proj.weight.data.add_(model.model.layers[diff_lay].self_attn.q_proj.weight.data.clone().to(1) - model_copy.model.layers[merge_base_lay].self_attn.q_proj.weight.data)\n#         model_copy.model.layers[merge_base_lay].self_attn.k_proj.weight.data.add_(model.model.layers[diff_lay].self_attn.k_proj.weight.data.clone().to(1) - model_copy.model.layers[merge_base_lay].self_attn.k_proj.weight.data)\n#         model_copy.model.layers[merge_base_lay].self_attn.v_proj.weight.data.add_(model.model.layers[diff_lay].self_attn.v_proj.weight.data.clone().to(1) - model_copy.model.layers[merge_base_lay].self_attn.v_proj.weight.data)\n#         model_copy.model.layers[merge_base_lay].self_attn.o_proj.weight.data.add_(model.model.layers[diff_lay].self_attn.o_proj.weight.data.clone().to(1) - model_copy.model.layers[merge_base_lay].self_attn.o_proj.weight.data)\n    \n#     # Delete the merged layers\n#     for diff_lay in range(merge_base_lay + merge_layer_num, merge_base_lay, -1):\n#         del model_copy.model.layers[diff_lay]\n\n#     del model\n#     gc.collect()\n#     return model_copy\n\ndef log_results(results, filename):\n    \"\"\"Appends experiment results to a CSV file.\"\"\"\n    file_exists = os.path.isfile(filename)\n    with open(filename, 'a', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=results.keys())\n        if not file_exists:\n            writer.writeheader()\n        writer.writerow(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T12:39:49.945981Z","iopub.execute_input":"2025-08-22T12:39:49.946716Z","iopub.status.idle":"2025-08-22T12:39:49.965346Z","shell.execute_reply.started":"2025-08-22T12:39:49.946680Z","shell.execute_reply":"2025-08-22T12:39:49.964491Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import gc\nimport torch\n\n\nimport gc\nimport torch\n\ndef merge_layers_return_model(model, merge_base_lay, merge_layer_num, temp_device=None):\n    \"\"\"\n    In-place layer merge without deepcopy.\n    Copies only needed layers to a temporary device (or same as base layer if None).\n    \"\"\"\n    total_layers = len(model.model.layers)\n    merge_layer_num = min(merge_layer_num, total_layers - merge_base_lay - 1)\n    if merge_layer_num <= 0:\n        return model\n\n    base_layer = model.model.layers[merge_base_lay]\n    base_device = base_layer.mlp.gate_proj.weight.device\n    temp_device = temp_device or base_device  # default to base layer's device\n\n    for diff_lay in range(merge_base_lay + 1, merge_base_lay + 1 + merge_layer_num):\n        # Clone each source layer to temp_device\n        src_mlp_gate = model.model.layers[diff_lay].mlp.gate_proj.weight.data.clone().to(temp_device)\n        src_mlp_down = model.model.layers[diff_lay].mlp.down_proj.weight.data.clone().to(temp_device)\n        src_mlp_up   = model.model.layers[diff_lay].mlp.up_proj.weight.data.clone().to(temp_device)\n\n        src_q = model.model.layers[diff_lay].self_attn.q_proj.weight.data.clone().to(temp_device)\n        src_k = model.model.layers[diff_lay].self_attn.k_proj.weight.data.clone().to(temp_device)\n        src_v = model.model.layers[diff_lay].self_attn.v_proj.weight.data.clone().to(temp_device)\n        src_o = model.model.layers[diff_lay].self_attn.o_proj.weight.data.clone().to(temp_device)\n\n        # Ensure base layer is also on temp_device before merging\n        base_layer = base_layer.to(temp_device)\n\n        # Merge MLP\n        base_layer.mlp.gate_proj.weight.data.add_(src_mlp_gate - base_layer.mlp.gate_proj.weight.data)\n        base_layer.mlp.down_proj.weight.data.add_(src_mlp_down - base_layer.mlp.down_proj.weight.data)\n        base_layer.mlp.up_proj.weight.data.add_(src_mlp_up - base_layer.mlp.up_proj.weight.data)\n\n        # Merge Attention\n        base_layer.self_attn.q_proj.weight.data.add_(src_q - base_layer.self_attn.q_proj.weight.data)\n        base_layer.self_attn.k_proj.weight.data.add_(src_k - base_layer.self_attn.k_proj.weight.data)\n        base_layer.self_attn.v_proj.weight.data.add_(src_v - base_layer.self_attn.v_proj.weight.data)\n        base_layer.self_attn.o_proj.weight.data.add_(src_o - base_layer.self_attn.o_proj.weight.data)\n\n        # Optionally move base layer back to its original device after each merge\n        if temp_device != base_device:\n            base_layer = base_layer.to(base_device)\n\n        del src_mlp_gate, src_mlp_down, src_mlp_up, src_q, src_k, src_v, src_o\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    # Remove merged layers\n    for idx in range(merge_base_lay + merge_layer_num, merge_base_lay, -1):\n        del model.model.layers[idx]\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T12:39:53.617836Z","iopub.execute_input":"2025-08-22T12:39:53.618120Z","iopub.status.idle":"2025-08-22T12:39:53.627308Z","shell.execute_reply.started":"2025-08-22T12:39:53.618099Z","shell.execute_reply":"2025-08-22T12:39:53.626714Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# --- Main Loop ---\n\nfor i, config in enumerate(hyperparameter_grid):\n    \n    experiment_start_time = time.time()\n    \n    print(\"\\n\" + \"=\"*50)\n    print(f\"Running Experiment {i+1}/{len(hyperparameter_grid)}\")\n    print(f\"Configuration: {config}\")\n    print(\"=\"*50)\n\n    try:\n        # 1. Set seed for reproducibility\n        set_seed(RANDOM_SEED)\n\n        # 2. Model Creation\n        base_model, tokenizer = create_model(config['model_path'], device=device_main)\n        \n        # Get original model stats\n        original_size = get_model_size(base_model)\n        original_layers = len(base_model.model.layers)\n\n        print('Base model created')\n\n        # 3. Model Compression (The core \"training\" or modification step)\n        print(\"\\n--- Starting Model Compression ---\")\n        compressed_model = compress_model(base_model, tokenizer, config)\n        print(\"--- Model Compression Finished ---\")\n\n        # 4. Evaluation\n        print(\"\\n--- Evaluating Compressed Model ---\")\n        perplexity = evaluate_perplexity(compressed_model, tokenizer, SMOKE_TEST, SMOKE_TEST_SAMPLES)\n        print(f\"Perplexity: {perplexity:.4f}\")\n        \n        # 5. Model Size Calculation\n        compressed_size = get_model_size(compressed_model)\n        compressed_layers = len(compressed_model.model.layers)\n        \n        # 6. Timing\n        experiment_end_time = time.time()\n        duration_minutes = (experiment_end_time - experiment_start_time) / 60\n        \n        # 7. Results Logging\n        results = {\n            'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            'model_path': config['model_path'],\n            'merge_layers': config['merge_layers'],\n            'interval': config['interval'],\n            'threshold': config['threshold'],\n            'original_layers': original_layers,\n            'compressed_layers': compressed_layers,\n            'original_params_B': f\"{original_size / 1e9:.3f}\",\n            'compressed_params_B': f\"{compressed_size / 1e9:.3f}\",\n            'perplexity': f\"{perplexity:.4f}\",\n            'duration_minutes': f\"{duration_minutes:.2f}\"\n        }\n        \n        log_results(results, RESULTS_FILE)\n        print(f\"\\nResults logged to {RESULTS_FILE}\")\n\n        # 8. Save Model\n        if SAVE_MODELS:\n            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            save_path = os.path.join(MODEL_SAVE_DIR, f'exp_{i+1}_{timestamp}')\n            os.makedirs(save_path, exist_ok=True)\n            compressed_model.save_pretrained(save_path)\n            tokenizer.save_pretrained(save_path)\n            print(f\"Model saved to {save_path}\")\n\n    except Exception as e:\n        print(f\"\\n!!!!!! Experiment {i+1} failed with an error: {e} !!!!!!\")\n        # Log failure\n        error_results = {\n            'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            'model_path': config.get('model_path', 'N/A'),\n            'merge_layers': config.get('merge_layers', 'N/A'),\n            'interval': config.get('interval', 'N/A'),\n            'threshold': config.get('threshold', 'N/A'),\n            'perplexity': 'FAILED',\n            'error_message': str(e)\n        }\n        log_results(error_results, RESULTS_FILE)\n        \n    finally:\n        # Clean up memory\n        del base_model, tokenizer, compressed_model\n        gc.collect()\n        torch.cuda.empty_cache()\n        print(\"\\nMemory cleaned up.\")\n\nprint(\"\\n\\nAll experiments finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T12:40:00.954077Z","iopub.execute_input":"2025-08-22T12:40:00.954393Z","iopub.status.idle":"2025-08-22T12:41:34.553994Z","shell.execute_reply.started":"2025-08-22T12:40:00.954366Z","shell.execute_reply":"2025-08-22T12:41:34.553244Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nRunning Experiment 1/3\nConfiguration: {'model_path': 'lmsys/vicuna-7b-v1.5', 'merge_layers': 4, 'interval': 2, 'highest_lay': 20, 'lowest_lay': 6, 'threshold': 0.65}\n==================================================\nLoading model: lmsys/vicuna-7b-v1.5\n","output_type":"stream"},{"name":"stderr","text":"2025-08-22 12:40:03.140598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755866403.163641     168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755866403.170708     168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e5dc8048f4d4a92b0768f7b229e404d"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Base model created\n\n--- Starting Model Compression ---\nCompressed model loaded\nAttempting to merge at layer: 16\nSimilarity after potential merge at layer 16: 0.7098\nMerge accepted. New layer count: 29\nAttempting to merge at layer: 14\nSimilarity after potential merge at layer 14: 0.4530\nMerge rejected.\nAttempting to merge at layer: 13\nSimilarity after potential merge at layer 13: 0.2012\nMerge rejected.\nAttempting to merge at layer: 12\nSimilarity after potential merge at layer 12: -0.0990\nMerge rejected.\nAttempting to merge at layer: 11\nSimilarity after potential merge at layer 11: -0.2110\nMerge rejected.\nAttempting to merge at layer: 10\nSimilarity after potential merge at layer 10: -0.3283\nMerge rejected.\nAttempting to merge at layer: 9\nSimilarity after potential merge at layer 9: 0.3934\nMerge rejected.\nAttempting to merge at layer: 8\nSimilarity after potential merge at layer 8: 0.5570\nMerge rejected.\nAttempting to merge at layer: 7\nSimilarity after potential merge at layer 7: 0.5578\nMerge rejected.\nAttempting to merge at layer: 6\nSimilarity after potential merge at layer 6: 0.5570\nMerge rejected.\n--- Model Compression Finished ---\n\n--- Evaluating Compressed Model ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3e085068b7542b08556174c6eb11bf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fde4e8d6ede438f8677d371d9fb1796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff3a03e4b7a42f7875c2d2faa532d5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a571a267dab4266bb9fd538b83a5375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4410d15bdabc4ccd9b0f91b57aa0b920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76907de858f04013bb66e972d3a84a62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7461ce4a94d648c5909057ebe4d935fb"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (341469 > 4096). Running this sequence through the model will result in indexing errors\nPerplexity Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  8.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: inf\n\nResults logged to results.csv\n\nMemory cleaned up.\n\n==================================================\nRunning Experiment 2/3\nConfiguration: {'model_path': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'merge_layers': 4, 'interval': 2, 'highest_lay': 39, 'lowest_lay': 0, 'threshold': 0.75}\n==================================================\nLoading model: HuggingFaceTB/SmolLM2-135M-Instruct\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4570b6297484fc682903c9f7153a0a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7711595cb5c7468b8e0026ef79ab09db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc1847026aa8447f87cdad7c6202ac8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2619e05d8b44221954b1cb09ded01cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15129799815348e6b2a7687473dd0050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9ad79612ce84782bfa19a38666a20da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"888c52cd4d754f5ebf5570a28d32cd43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f613672d21f148bcb999ebcd64b1ae6d"}},"metadata":{}},{"name":"stdout","text":"Base model created\n\n--- Starting Model Compression ---\nCompressed model loaded\nAttempting to merge at layer: 35\nAttempting to merge at layer: 25\nSimilarity after potential merge at layer 25: 0.7132\nMerge rejected.\nAttempting to merge at layer: 24\nSimilarity after potential merge at layer 24: 0.5793\nMerge rejected.\nAttempting to merge at layer: 23\nSimilarity after potential merge at layer 23: 0.5768\nMerge rejected.\nAttempting to merge at layer: 22\nSimilarity after potential merge at layer 22: 0.5755\nMerge rejected.\nAttempting to merge at layer: 21\nSimilarity after potential merge at layer 21: 0.5730\nMerge rejected.\nAttempting to merge at layer: 20\nSimilarity after potential merge at layer 20: 0.5724\nMerge rejected.\nAttempting to merge at layer: 19\nSimilarity after potential merge at layer 19: 0.5720\nMerge rejected.\nAttempting to merge at layer: 18\nSimilarity after potential merge at layer 18: 0.5710\nMerge rejected.\nAttempting to merge at layer: 17\nSimilarity after potential merge at layer 17: 0.5704\nMerge rejected.\nAttempting to merge at layer: 16\nSimilarity after potential merge at layer 16: 0.5706\nMerge rejected.\nAttempting to merge at layer: 15\nSimilarity after potential merge at layer 15: 0.5702\nMerge rejected.\nAttempting to merge at layer: 14\nSimilarity after potential merge at layer 14: 0.5694\nMerge rejected.\nAttempting to merge at layer: 13\nSimilarity after potential merge at layer 13: 0.5693\nMerge rejected.\nAttempting to merge at layer: 12\nSimilarity after potential merge at layer 12: 0.5690\nMerge rejected.\nAttempting to merge at layer: 11\nSimilarity after potential merge at layer 11: 0.0917\nMerge rejected.\nAttempting to merge at layer: 10\nSimilarity after potential merge at layer 10: 0.1897\nMerge rejected.\nAttempting to merge at layer: 9\nSimilarity after potential merge at layer 9: 0.3316\nMerge rejected.\nAttempting to merge at layer: 8\nSimilarity after potential merge at layer 8: 0.3224\nMerge rejected.\nAttempting to merge at layer: 7\nSimilarity after potential merge at layer 7: 0.3032\nMerge rejected.\nAttempting to merge at layer: 6\nSimilarity after potential merge at layer 6: 0.3180\nMerge rejected.\nAttempting to merge at layer: 5\nSimilarity after potential merge at layer 5: 0.3129\nMerge rejected.\nAttempting to merge at layer: 4\nSimilarity after potential merge at layer 4: 0.3129\nMerge rejected.\nAttempting to merge at layer: 3\nSimilarity after potential merge at layer 3: 0.3078\nMerge rejected.\nAttempting to merge at layer: 2\nSimilarity after potential merge at layer 2: 0.1896\nMerge rejected.\nAttempting to merge at layer: 1\nSimilarity after potential merge at layer 1: 0.0728\nMerge rejected.\nAttempting to merge at layer: 0\nSimilarity after potential merge at layer 0: 0.0706\nMerge rejected.\n--- Model Compression Finished ---\n\n--- Evaluating Compressed Model ---\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (304978 > 8192). Running this sequence through the model will result in indexing errors\nPerplexity Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 378.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: inf\n\nResults logged to results.csv\n\nMemory cleaned up.\n\n==================================================\nRunning Experiment 3/3\nConfiguration: {'model_path': 'lmsys/vicuna-7b-v1.5', 'merge_layers': 3, 'interval': 3, 'highest_lay': 39, 'lowest_lay': 0, 'threshold': 0.65}\n==================================================\nLoading model: lmsys/vicuna-7b-v1.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b707c70cf964c9b929e36c3f7595fe5"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Base model created\n\n--- Starting Model Compression ---\nCompressed model loaded\nAttempting to merge at layer: 36\nAttempting to merge at layer: 28\nSimilarity after potential merge at layer 28: 0.7931\nMerge accepted. New layer count: 30\nAttempting to merge at layer: 25\nSimilarity after potential merge at layer 25: 0.7596\nMerge accepted. New layer count: 28\nAttempting to merge at layer: 22\nSimilarity after potential merge at layer 22: 0.6693\nMerge accepted. New layer count: 26\nAttempting to merge at layer: 19\nSimilarity after potential merge at layer 19: 0.4258\nMerge rejected.\nAttempting to merge at layer: 18\nSimilarity after potential merge at layer 18: 0.2088\nMerge rejected.\nAttempting to merge at layer: 17\nSimilarity after potential merge at layer 17: 0.1237\nMerge rejected.\nAttempting to merge at layer: 16\nSimilarity after potential merge at layer 16: 0.5653\nMerge rejected.\nAttempting to merge at layer: 15\nSimilarity after potential merge at layer 15: 0.5779\nMerge rejected.\nAttempting to merge at layer: 14\nSimilarity after potential merge at layer 14: 0.5717\nMerge rejected.\nAttempting to merge at layer: 13\nSimilarity after potential merge at layer 13: 0.5666\nMerge rejected.\nAttempting to merge at layer: 12\nSimilarity after potential merge at layer 12: 0.5598\nMerge rejected.\nAttempting to merge at layer: 11\nSimilarity after potential merge at layer 11: 0.5579\nMerge rejected.\nAttempting to merge at layer: 10\nSimilarity after potential merge at layer 10: 0.5577\nMerge rejected.\nAttempting to merge at layer: 9\nSimilarity after potential merge at layer 9: 0.5574\nMerge rejected.\nAttempting to merge at layer: 8\nSimilarity after potential merge at layer 8: 0.5570\nMerge rejected.\nAttempting to merge at layer: 7\nSimilarity after potential merge at layer 7: 0.5578\nMerge rejected.\nAttempting to merge at layer: 6\nSimilarity after potential merge at layer 6: 0.5570\nMerge rejected.\nAttempting to merge at layer: 5\nSimilarity after potential merge at layer 5: 0.5600\nMerge rejected.\nAttempting to merge at layer: 4\nSimilarity after potential merge at layer 4: 0.5603\nMerge rejected.\nAttempting to merge at layer: 3\nSimilarity after potential merge at layer 3: 0.5584\nMerge rejected.\nAttempting to merge at layer: 2\nSimilarity after potential merge at layer 2: 0.5607\nMerge rejected.\nAttempting to merge at layer: 1\nSimilarity after potential merge at layer 1: -0.0744\nMerge rejected.\nAttempting to merge at layer: 0\nSimilarity after potential merge at layer 0: -0.4069\nMerge rejected.\n--- Model Compression Finished ---\n\n--- Evaluating Compressed Model ---\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (341469 > 4096). Running this sequence through the model will result in indexing errors\nPerplexity Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 358.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: inf\n\nResults logged to results.csv\n\nMemory cleaned up.\n\n\nAll experiments finished.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Display the results table\nprint(\"\\n--- Experiment Results Summary ---\")\nif os.path.exists(RESULTS_FILE):\n    results_df = pd.read_csv(RESULTS_FILE)\n    display(results_df)\nelse:\n    print(f\"Results file '{RESULTS_FILE}' not found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T12:41:38.084755Z","iopub.execute_input":"2025-08-22T12:41:38.085418Z","iopub.status.idle":"2025-08-22T12:41:38.329876Z","shell.execute_reply.started":"2025-08-22T12:41:38.085388Z","shell.execute_reply":"2025-08-22T12:41:38.328940Z"}},"outputs":[{"name":"stdout","text":"\n--- Experiment Results Summary ---\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_168/3913139564.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Experiment Results Summary ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESULTS_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESULTS_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 7 fields in line 4, saw 11\n"],"ename":"ParserError","evalue":"Error tokenizing data. C error: Expected 7 fields in line 4, saw 11\n","output_type":"error"}],"execution_count":6}]}